{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a47d9-8778-4db5-94be-173846cbd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import pearsonr, gaussian_kde, linregress, ttest_ind, sem, zscore\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.utils.validation import check_random_state\n",
    "from math import factorial\n",
    "from more_itertools import distinct_permutations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, ParameterGrid, train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "#from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from scipy.stats import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from colorama import Fore, Style, init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from scipy.stats import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from colorama import Fore, Style, init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabecce3-4309-4dda-8aea-2e7a0ecfa21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def summarize_feature(df, feature, df_out, stat_cols=(10, 50), df_offset=2):\n",
    "\n",
    "    d = df[df[\"Feature\"] == feature]\n",
    "    if d.empty:\n",
    "        return df_out  \n",
    "\n",
    "    for k in stat_cols:\n",
    "        col = f\"hl_statistic_{k}\"\n",
    "        if col in d.columns:\n",
    "            df_chi = k - df_offset\n",
    "            combined_p = 1 - chi2.cdf(d[col].mean(), df_chi)\n",
    "            df_out.loc[feature, f\"hl_pvalue_combined_{k}\"] = combined_p\n",
    "\n",
    "    # --- OR summary ---\n",
    "    if \"OR\" in d.columns:\n",
    "        or_mean = d[\"OR\"].mean()\n",
    "        or_std = d[\"OR\"].std(ddof=1)\n",
    "        df_out.loc[feature, \"OR\"] = or_mean\n",
    "        df_out.loc[feature, \"2.5%\"] = or_mean - or_std\n",
    "        df_out.loc[feature, \"97.5%\"] = or_mean + or_std\n",
    "\n",
    "    # --- min/max de los IC originales (tal como lo estabas haciendo) ---\n",
    "    if \"2.5%\" in d.columns:\n",
    "        df_out.loc[feature, \"minOR\"] = d[\"2.5%\"].min()\n",
    "    if \"97.5%\" in d.columns:\n",
    "        df_out.loc[feature, \"maxOR\"] = d[\"97.5%\"].max()\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "features = [\"Mono\", \"One\", \"Two\", \"Three\", \"Total\"] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hosmer_lemeshow(y_true, y_prob, g):\n",
    "    hl_df = pd.DataFrame({\n",
    "        \"observed\": y_true,\n",
    "        \"predicted_probability\": y_prob\n",
    "    }).dropna()\n",
    "\n",
    "\n",
    "    hl_df[\"group\"] = pd.qcut(hl_df[\"predicted_probability\"], g, duplicates=\"drop\")\n",
    "\n",
    "    hl_table = hl_df.groupby(\"group\").apply(\n",
    "        lambda x: pd.Series({\n",
    "            \"observed\": x[\"observed\"].sum(),\n",
    "            \"expected\": x[\"predicted_probability\"].sum(),\n",
    "            \"total\": len(x)\n",
    "        })\n",
    "    )\n",
    "\n",
    "    hl_table[\"observed_neg\"] = hl_table[\"total\"] - hl_table[\"observed\"]\n",
    "    hl_table[\"expected_neg\"] = hl_table[\"total\"] - hl_table[\"expected\"]\n",
    "\n",
    "\n",
    "    hl_statistic = (\n",
    "        ((hl_table[\"observed\"] - hl_table[\"expected\"])**2) / hl_table[\"expected\"] +\n",
    "        ((hl_table[\"observed_neg\"] - hl_table[\"expected_neg\"])**2) / hl_table[\"expected_neg\"]\n",
    "    ).sum()\n",
    "\n",
    "\n",
    "    dof = hl_table.shape[0] - 2\n",
    "    p_value = 1 - chi2.cdf(hl_statistic, dof)\n",
    "\n",
    "    return hl_statistic, p_value\n",
    "\n",
    "\n",
    "\n",
    "def summarize_feature_by_covar(df, covar, feature, df_out, stat_cols=(10, 50), df_offset=2):\n",
    "    d = df[(df[\"Covar\"] == covar) & (df[\"Feature\"] == feature)]\n",
    "    if d.empty:\n",
    "        return df_out\n",
    "\n",
    "    idx = (covar, feature)\n",
    "\n",
    "    for k in stat_cols:\n",
    "        col = f\"hl_statistic_{k}\"\n",
    "        if col in d.columns:\n",
    "            df_chi = k - df_offset\n",
    "            df_out.loc[idx, f\"hl_pvalue_combined_{k}\"] = 1 - chi2.cdf(d[col].mean(), df_chi)\n",
    "\n",
    "    or_mean = d[\"OR\"].mean()\n",
    "    or_std  = d[\"OR\"].std(ddof=1)\n",
    "    df_out.loc[idx, \"OR\"] = or_mean\n",
    "    df_out.loc[idx, \"2.5%\"] = or_mean - or_std\n",
    "    df_out.loc[idx, \"97.5%\"] = or_mean + or_std\n",
    "\n",
    "    df_out.loc[idx, \"minOR\"] = d[\"2.5%\"].min()\n",
    "    df_out.loc[idx, \"maxOR\"] = d[\"97.5%\"].max()\n",
    "\n",
    "    df_out.loc[idx, \"n_iter\"] = d.shape[0]\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b0264-ed25-4657-8942-47b1e1e26af9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae4bf3-d7cb-4505-b2e5-63c1daa3e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd699022-b5ce-4098-a5c8-cef93fec309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/BBAG-cross.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abb9a9-c9f3-4578-a857-1a77d76b90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.country != 'Slovakia'].reset_index(drop =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6d1fc-81a3-4ef8-9617-7ce68ed50718",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00dd336-570a-4863-8c9c-e9aaf8cd55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ = ['Mono', 'One',\t'Two',\t'Three', 'Total']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd132d-d825-4768-9a67-4326132e3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[vars_] = data[vars_] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da01889-bf76-4bb4-9e3b-de9c0d5ea14b",
   "metadata": {},
   "source": [
    "# Odds ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993fa9c-3a22-4278-a618-1b62601dcd75",
   "metadata": {},
   "source": [
    "## Without co-vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d605d-d8cc-44e2-9ada-1ef18e799242",
   "metadata": {},
   "source": [
    "### Without iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126aec1-9f4b-4ec6-b8c3-48b83d23372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merge_df_all = data.copy()\n",
    "\n",
    "results_merge_df_all = results_merge_df_all.loc[:, ~results_merge_df_all.columns.duplicated()]\n",
    "\n",
    "df_directions_odd = pd.DataFrame()\n",
    "\n",
    "for i in vars_:\n",
    "    \n",
    "    y_ols = results_merge_df_all['GAP_bin']\n",
    "\n",
    "    \n",
    "    X_ols = results_merge_df_all[[i]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ols, y_ols, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    X_train['intercept'] = 1\n",
    "    X_test['intercept'] = 1\n",
    "    \n",
    "    model = sm.Logit(y_train, X_train).fit(disp = 0)\n",
    "    \n",
    "    params = model.params\n",
    "    conf = np.exp(model.conf_int())\n",
    "    conf['OR'] = np.exp(params)\n",
    "    conf['z'] =model.tvalues\n",
    "    conf['P>|z|'] =model.pvalues\n",
    "    conf.columns = ['2.5%', '97.5%', 'OR', 'z', 'P>|z|']\n",
    "    \n",
    "    df = conf.loc[i:i]\n",
    "    df['Feature'] = i\n",
    "    df_directions_odd = pd.concat([df_directions_odd, df])\n",
    "\n",
    "df_directions_odd = df_directions_odd.reset_index(drop=True)\n",
    "\n",
    "df_directions_odd.to_excel('Results/cross/cross_OR_-removing-slovakia-proportions.xlsx')\n",
    "df_directions_odd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4eea6-6a09-48ee-ae93-fd79b28fdfc1",
   "metadata": {},
   "source": [
    "### 1000-iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd81ab7-fdac-44d9-8aa2-3b661ee65f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "results_dict = {}\n",
    "for i in vars_:\n",
    "    results_dict[i + \"_y\"] = pd.Series(dtype=float)\n",
    "    results_dict[i + \"_ypred\"] = pd.Series(dtype=float)\n",
    "\n",
    "results_merge_df_all = data.copy()\n",
    "results_merge_df_all.dropna(subset=vars_ + [\"GAP_bin\"], inplace=True)\n",
    "results_merge_df_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_directions_odd = pd.DataFrame()\n",
    "\n",
    "n_boosts = 10  #For performance reasons, the number of iterations was set to 10; however, 1,000 iterations were used for the results reported in the manuscript.â€\n",
    "test_size = 500 / results_merge_df_all.shape[0]\n",
    "\n",
    "for boosts in range(n_boosts):\n",
    "    for i in vars_:\n",
    "\n",
    "        y_ols = results_merge_df_all[\"GAP_bin\"]\n",
    "        X_ols = results_merge_df_all[[i]]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_ols, y_ols,\n",
    "            test_size=test_size,\n",
    "            stratify=results_merge_df_all[\"GAP_bin\"],\n",
    "            random_state=boosts\n",
    "        )\n",
    "\n",
    "        \n",
    "        X_train[\"intercept\"] = 1\n",
    "        X_test[\"intercept\"] = 1\n",
    "\n",
    "\n",
    "        model = sm.Logit(y_train, X_train).fit(disp=0)\n",
    "\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        results_dict[i + \"_y\"] = pd.concat([results_dict[i + \"_y\"], y_test], axis=0)\n",
    "        results_dict[i + \"_ypred\"] = pd.concat([results_dict[i + \"_ypred\"], y_test_pred], axis=0)\n",
    "\n",
    "        params = model.params\n",
    "        conf = np.exp(model.conf_int())\n",
    "        conf[\"OR\"] = np.exp(params)\n",
    "        conf[\"z\"] = model.tvalues\n",
    "        conf[\"P>|z|\"] = model.pvalues\n",
    "        conf.columns = [\"2.5%\", \"97.5%\", \"OR\", \"z\", \"P>|z|\"]\n",
    "\n",
    "        df = conf.loc[i:i].copy()\n",
    "        df[\"Feature\"] = i\n",
    "\n",
    "        hl10_stat, hl10_p = hosmer_lemeshow(y_test, y_test_pred, g=10)\n",
    "        hl50_stat, hl50_p = hosmer_lemeshow(y_test, y_test_pred, g=50)\n",
    "        hl100_stat, hl100_p = hosmer_lemeshow(y_test, y_test_pred, g=100)\n",
    "\n",
    "        df[\"hl_statistic_10\"] = hl10_stat\n",
    "        df[\"hl_pvalue_10\"] = hl10_p\n",
    "\n",
    "        df[\"hl_statistic_50\"] = hl50_stat\n",
    "        df[\"hl_pvalue_50\"] = hl50_p\n",
    "\n",
    "        df[\"hl_statistic_100\"] = hl100_stat\n",
    "        df[\"hl_pvalue_100\"] = hl100_p\n",
    "\n",
    "        df_directions_odd = pd.concat([df_directions_odd, df], axis=0)\n",
    "\n",
    "    df_directions_odd = df_directions_odd.reset_index(drop=True)\n",
    "\n",
    "df_directions_odd;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7a9a3-6ded-41c7-8389-2a5c1c488459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_directions_odd_f = pd.DataFrame()\n",
    "\n",
    "for f in vars_:\n",
    "    df_directions_odd_f = summarize_feature(df_directions_odd, f, df_directions_odd_f, stat_cols=(10, 50, 100))\n",
    "\n",
    "df_directions_odd_f.to_excel('Results/cross/cross_OR_-removing-slovakia-proportions-iter.xlsx')\n",
    "df_directions_odd_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45dbb7-5acb-493a-b814-af53e9c919df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe9540-2a54-4c73-bea0-ffbd80edb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time.perf_counter() - t0\n",
    "m, s = divmod(dt, 60)\n",
    "print(f\"Tiempo total: {int(m)} min {s:.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713800c-8ecd-47cc-a1b2-c752797506e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
