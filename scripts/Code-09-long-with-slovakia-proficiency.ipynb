{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a47d9-8778-4db5-94be-173846cbd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import pearsonr, gaussian_kde, linregress, ttest_ind, sem, zscore\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.utils.validation import check_random_state\n",
    "from math import factorial\n",
    "from more_itertools import distinct_permutations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, ParameterGrid, train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "#from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from scipy.stats import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from colorama import Fore, Style, init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from scipy.stats import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from colorama import Fore, Style, init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabecce3-4309-4dda-8aea-2e7a0ecfa21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def summarize_feature(df, feature, df_out, stat_cols=(10, 50), df_offset=2):\n",
    "\n",
    "    d = df[df[\"Feature\"] == feature]\n",
    "    if d.empty:\n",
    "        return df_out  \n",
    "\n",
    "    for k in stat_cols:\n",
    "        col = f\"hl_statistic_{k}\"\n",
    "        if col in d.columns:\n",
    "            df_chi = k - df_offset\n",
    "            combined_p = 1 - chi2.cdf(d[col].mean(), df_chi)\n",
    "            df_out.loc[feature, f\"hl_pvalue_combined_{k}\"] = combined_p\n",
    "\n",
    "    # --- RR summary ---\n",
    "    if \"RR\" in d.columns:\n",
    "        or_mean = d[\"RR\"].mean()\n",
    "        or_std = d[\"RR\"].std(ddof=1)\n",
    "        df_out.loc[feature, \"RR\"] = or_mean\n",
    "        df_out.loc[feature, \"2.5%\"] = or_mean - or_std\n",
    "        df_out.loc[feature, \"97.5%\"] = or_mean + or_std\n",
    "\n",
    "    # --- min/max de los IC originales (tal como lo estabas haciendo) ---\n",
    "    if \"2.5%\" in d.columns:\n",
    "        df_out.loc[feature, \"minRR\"] = d[\"2.5%\"].min()\n",
    "    if \"97.5%\" in d.columns:\n",
    "        df_out.loc[feature, \"maxRR\"] = d[\"97.5%\"].max()\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "features = [\"Mono\", \"One\", \"Two\", \"Three\", \"Total\"] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hosmer_lemeshow(y_true, y_prob, g):\n",
    "    hl_df = pd.DataFrame({\n",
    "        \"observed\": y_true,\n",
    "        \"predicted_probability\": y_prob\n",
    "    }).dropna()\n",
    "\n",
    "\n",
    "    hl_df[\"group\"] = pd.qcut(hl_df[\"predicted_probability\"], g, duplicates=\"drop\")\n",
    "\n",
    "    hl_table = hl_df.groupby(\"group\").apply(\n",
    "        lambda x: pd.Series({\n",
    "            \"observed\": x[\"observed\"].sum(),\n",
    "            \"expected\": x[\"predicted_probability\"].sum(),\n",
    "            \"total\": len(x)\n",
    "        })\n",
    "    )\n",
    "\n",
    "    hl_table[\"observed_neg\"] = hl_table[\"total\"] - hl_table[\"observed\"]\n",
    "    hl_table[\"expected_neg\"] = hl_table[\"total\"] - hl_table[\"expected\"]\n",
    "\n",
    "\n",
    "    hl_statistic = (\n",
    "        ((hl_table[\"observed\"] - hl_table[\"expected\"])**2) / hl_table[\"expected\"] +\n",
    "        ((hl_table[\"observed_neg\"] - hl_table[\"expected_neg\"])**2) / hl_table[\"expected_neg\"]\n",
    "    ).sum()\n",
    "\n",
    "\n",
    "    dof = hl_table.shape[0] - 2\n",
    "    p_value = 1 - chi2.cdf(hl_statistic, dof)\n",
    "\n",
    "    return hl_statistic, p_value\n",
    "\n",
    "\n",
    "\n",
    "def summarize_feature_by_covar(df, covar, feature, df_out, stat_cols=(10, 50), df_offset=2):\n",
    "    d = df[(df[\"Covar\"] == covar) & (df[\"Feature\"] == feature)]\n",
    "    if d.empty:\n",
    "        return df_out\n",
    "\n",
    "    idx = (covar, feature)\n",
    "\n",
    "    for k in stat_cols:\n",
    "        col = f\"hl_statistic_{k}\"\n",
    "        if col in d.columns:\n",
    "            df_chi = k - df_offset\n",
    "            df_out.loc[idx, f\"hl_pvalue_combined_{k}\"] = 1 - chi2.cdf(d[col].mean(), df_chi)\n",
    "\n",
    "    or_mean = d[\"RR\"].mean()\n",
    "    or_std  = d[\"RR\"].std(ddof=1)\n",
    "    df_out.loc[idx, \"RR\"] = or_mean\n",
    "    df_out.loc[idx, \"2.5%\"] = or_mean - or_std\n",
    "    df_out.loc[idx, \"97.5%\"] = or_mean + or_std\n",
    "\n",
    "    df_out.loc[idx, \"minRR\"] = d[\"2.5%\"].min()\n",
    "    df_out.loc[idx, \"maxRR\"] = d[\"97.5%\"].max()\n",
    "\n",
    "    df_out.loc[idx, \"n_iter\"] = d.shape[0]\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b0264-ed25-4657-8942-47b1e1e26af9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae4bf3-d7cb-4505-b2e5-63c1daa3e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd699022-b5ce-4098-a5c8-cef93fec309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/BBAG-long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abb9a9-c9f3-4578-a857-1a77d76b90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00dd336-570a-4863-8c9c-e9aaf8cd55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ = ['Proficiency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da01889-bf76-4bb4-9e3b-de9c0d5ea14b",
   "metadata": {},
   "source": [
    "# Relative Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993fa9c-3a22-4278-a618-1b62601dcd75",
   "metadata": {},
   "source": [
    "## Without co-vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d605d-d8cc-44e2-9ada-1ef18e799242",
   "metadata": {},
   "source": [
    "### Without iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126aec1-9f4b-4ec6-b8c3-48b83d23372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merge_df_all = data.copy()\n",
    "\n",
    "results_merge_df_all = results_merge_df_all.loc[:, ~results_merge_df_all.columns.duplicated()]\n",
    "\n",
    "df_directions_odd = pd.DataFrame()\n",
    "\n",
    "for i in vars_:\n",
    "    \n",
    "    y_ols = results_merge_df_all['GAP_bin']\n",
    "\n",
    "    \n",
    "    X_ols = results_merge_df_all[[i] + ['delta_time']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ols, y_ols, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    X_train_scaled['intercept'] = 1\n",
    "    X_test_scaled['intercept'] = 1\n",
    "    \n",
    "    model = sm.GLM(y_train, X_train_scaled, family=sm.families.Binomial(link=sm.families.links.log())).fit(disp = 0)\n",
    "    \n",
    "    params = model.params\n",
    "    conf = np.exp(model.conf_int())\n",
    "    conf['RR'] = np.exp(params)\n",
    "    conf['z'] =model.tvalues\n",
    "    conf['P>|z|'] =model.pvalues\n",
    "    conf.columns = ['2.5%', '97.5%', 'RR', 'z', 'P>|z|']\n",
    "    \n",
    "    df = conf.loc[i:i]\n",
    "    df['Feature'] = i\n",
    "    df_directions_odd = pd.concat([df_directions_odd, df])\n",
    "\n",
    "df_directions_odd = df_directions_odd.reset_index(drop=True)\n",
    "\n",
    "df_directions_odd.to_excel('Results/long/long_RR_-with-slovakia-proficiency.xlsx')\n",
    "df_directions_odd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4eea6-6a09-48ee-ae93-fd79b28fdfc1",
   "metadata": {},
   "source": [
    "### 1000-iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd81ab7-fdac-44d9-8aa2-3b661ee65f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "results_dict = {}\n",
    "for i in vars_:\n",
    "    results_dict[i + \"_y\"] = pd.Series(dtype=float)\n",
    "    results_dict[i + \"_ypred\"] = pd.Series(dtype=float)\n",
    "\n",
    "results_merge_df_all = data.copy()\n",
    "\n",
    "\n",
    "df_directions_odd = pd.DataFrame()\n",
    "\n",
    "n_boosts = 10  #For performance reasons, the number of iterations was set to 10; however, 1,000 iterations were used for the results reported in the manuscript.”\n",
    "test_size = 500 / results_merge_df_all.shape[0]\n",
    "\n",
    "for boosts in range(n_boosts):\n",
    "    for i in vars_:\n",
    "\n",
    "        y_ols = results_merge_df_all[\"GAP_bin\"]\n",
    "        X_ols = results_merge_df_all[[i] + ['delta_time']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_ols, y_ols,\n",
    "            test_size=test_size,\n",
    "            stratify=results_merge_df_all[\"GAP_bin\"],\n",
    "            random_state=boosts\n",
    "        )\n",
    "\n",
    "        scaler = MinMaxScaler((0.05, 0.95))\n",
    "\n",
    "        X_train_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train),\n",
    "            columns=X_train.columns,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_test),\n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "\n",
    "        X_train_scaled[\"intercept\"] = 1\n",
    "        X_test_scaled[\"intercept\"] = 1\n",
    "\n",
    "\n",
    "        model = sm.GLM(y_train, X_train_scaled, family=sm.families.Binomial(link=sm.families.links.log())).fit(disp = 0)\n",
    "\n",
    "\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        results_dict[i + \"_y\"] = pd.concat([results_dict[i + \"_y\"], y_test], axis=0)\n",
    "        results_dict[i + \"_ypred\"] = pd.concat([results_dict[i + \"_ypred\"], y_test_pred], axis=0)\n",
    "\n",
    "        params = model.params\n",
    "        conf = np.exp(model.conf_int())\n",
    "        conf[\"RR\"] = np.exp(params)\n",
    "        conf[\"z\"] = model.tvalues\n",
    "        conf[\"P>|z|\"] = model.pvalues\n",
    "        conf.columns = [\"2.5%\", \"97.5%\", \"RR\", \"z\", \"P>|z|\"]\n",
    "\n",
    "        df = conf.loc[i:i].copy()\n",
    "        df[\"Feature\"] = i\n",
    "\n",
    "        hl10_stat, hl10_p = hosmer_lemeshow(y_test, y_test_pred, g=10)\n",
    "        hl50_stat, hl50_p = hosmer_lemeshow(y_test, y_test_pred, g=50)\n",
    "        hl100_stat, hl100_p = hosmer_lemeshow(y_test, y_test_pred, g=100)\n",
    "\n",
    "        df[\"hl_statistic_10\"] = hl10_stat\n",
    "        df[\"hl_pvalue_10\"] = hl10_p\n",
    "\n",
    "        df[\"hl_statistic_50\"] = hl50_stat\n",
    "        df[\"hl_pvalue_50\"] = hl50_p\n",
    "\n",
    "        df[\"hl_statistic_100\"] = hl100_stat\n",
    "        df[\"hl_pvalue_100\"] = hl100_p\n",
    "\n",
    "        df_directions_odd = pd.concat([df_directions_odd, df], axis=0)\n",
    "\n",
    "    df_directions_odd = df_directions_odd.reset_index(drop=True)\n",
    "\n",
    "df_directions_odd;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7a9a3-6ded-41c7-8389-2a5c1c488459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_directions_odd_f = pd.DataFrame()\n",
    "\n",
    "for f in vars_:\n",
    "    df_directions_odd_f = summarize_feature(df_directions_odd, f, df_directions_odd_f, stat_cols=(10, 50, 100))\n",
    "\n",
    "df_directions_odd_f.to_excel('Results/long/long_RR_-with-slovakia-proficiency-iter.xlsx')\n",
    "df_directions_odd_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45dbb7-5acb-493a-b814-af53e9c919df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "456b8f59-faee-49ad-9213-03aff611ab61",
   "metadata": {},
   "source": [
    "## Odd ratios with macrosocials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c38ae1-0e19-4434-b75b-b284c0771fd1",
   "metadata": {},
   "source": [
    "### Without Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922b411-f7c5-4fc4-a305-2dc90d60fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_social_physical = ['gender_equal_l', 'Polution_conc_inv', 'Eq']\n",
    "\n",
    "vars_sociopolitical = ['free_parties_l',  'inclu_suff_est', 'cred_elect_est', 'local_dem_est']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30247528-c620-4b41-9343-a48c44a8406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merge_df_all = data.copy()\n",
    "\n",
    "covar_list = (\n",
    "    ['total_exposomes', 'sociopolitical', 'social_physical', 'GDP',\n",
    "     'number_leng_inst', 'number_stable_inst', 'distance', 'Migration']\n",
    "    + vars_social_physical\n",
    "    + vars_sociopolitical\n",
    ")\n",
    "\n",
    "df_directions_odd_all = []\n",
    "df_directions_cov_odd_all = []\n",
    "\n",
    "for covar in covar_list:\n",
    "    for i in vars_:\n",
    "\n",
    "        c_results_merge_df_all = results_merge_df_all.dropna(subset= [i, covar] + ['delta_time', 'GAP_bin'])\n",
    "        c_results_merge_df_all.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        y_ols = c_results_merge_df_all['GAP_bin']\n",
    "        X_ols = c_results_merge_df_all[[i, covar] + ['delta_time']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_ols, y_ols, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        scaler = MinMaxScaler((0.05, 0.95))\n",
    "\n",
    "        X_train_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train),\n",
    "            columns=X_train.columns,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_test),\n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "\n",
    "        X_train_scaled['intercept'] = 1\n",
    "        X_test_scaled['intercept'] = 1\n",
    "\n",
    "        model = sm.GLM(y_train, X_train_scaled, family=sm.families.Binomial(link=sm.families.links.log())).fit(disp = 0)\n",
    "\n",
    "        params = model.params\n",
    "        conf = np.exp(model.conf_int())\n",
    "        conf['RR'] = np.exp(params)\n",
    "        conf['z'] = model.tvalues\n",
    "        conf['P>|z|'] = model.pvalues\n",
    "        conf.columns = ['2.5%', '97.5%', 'RR', 'z', 'P>|z|']\n",
    "\n",
    "        df_i = conf.loc[i:i].copy()\n",
    "        df_i['Feature'] = i\n",
    "        df_i['Covar'] = covar\n",
    "        df_directions_odd_all.append(df_i)\n",
    "\n",
    "        df_c = conf.loc[covar:covar].copy()\n",
    "        df_c['Feature'] = f\"{i}({covar})\"\n",
    "        df_c['Exposure'] = i          # opcional pero útil\n",
    "        df_c['Covar'] = covar\n",
    "        df_directions_cov_odd_all.append(df_c)\n",
    "\n",
    "df_directions_odd = pd.concat(df_directions_odd_all, axis=0).reset_index(drop=True)\n",
    "df_directions_cov_odd = pd.concat(df_directions_cov_odd_all, axis=0).reset_index(drop=True)\n",
    "\n",
    "#df_directions_odd, df_directions_cov_odd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086557f3-ce36-4544-a24a-cabb3371451c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for covar in ['total_exposomes', 'sociopolitical', 'social_physical', 'GDP', 'number_leng_inst', 'number_stable_inst', 'distance', 'Migration'] + vars_social_physical + vars_sociopolitical:\n",
    "    c_dfRR = df_directions_odd[df_directions_odd.Covar == covar]\n",
    "    c_dfRR.to_excel('Results/long/long_RR_-with-slovakia-proficiency_covar-' + covar + '.xlsx')\n",
    "\n",
    "    print(covar)\n",
    "    display(c_dfRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4320d3b-b6c5-4dc6-bfb3-04c135d13172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eb54788-a8ad-4ffc-b897-df216754a1b7",
   "metadata": {},
   "source": [
    "### 1000-Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d49ea1-6e2c-4bf5-88f9-6c85d113263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_merge_df_all = data.copy()\n",
    "\n",
    "covar_list = (\n",
    "    ['total_exposomes', 'sociopolitical', 'social_physical', 'GDP',\n",
    "     'number_leng_inst', 'number_stable_inst', 'distance', 'Migration']\n",
    "    + vars_social_physical\n",
    "    + vars_sociopolitical\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "n_boosts = 10  #For performance reasons, the number of iterations was set to 10; however, 1,000 iterations were used for the results reported in the manuscript.”\n",
    "test_size = 500 / results_merge_df_all.shape[0]\n",
    "\n",
    "\n",
    "df_directions_odd_all = []\n",
    "df_directions_cov_all = []\n",
    "\n",
    "\n",
    "results_dict = {}  \n",
    "\n",
    "for covar in covar_list:\n",
    "    print(covar)\n",
    "    for boosts in range(n_boosts):\n",
    "\n",
    "        for i in vars_:\n",
    "\n",
    "            c_results_merge_df_all = results_merge_df_all.dropna(subset= [i, covar] + ['delta_time', 'GAP_bin'])\n",
    "            c_results_merge_df_all.reset_index(drop = True, inplace = True)\n",
    "            \n",
    "            y_ols = c_results_merge_df_all['GAP_bin']\n",
    "            X_ols = c_results_merge_df_all[[i, covar] + ['delta_time']]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_ols, y_ols,\n",
    "                test_size=test_size,\n",
    "                stratify=y_ols,\n",
    "                random_state=boosts\n",
    "            )\n",
    "\n",
    "            scaler = MinMaxScaler((0.05, 0.95))\n",
    "\n",
    "            X_train_scaled = pd.DataFrame(\n",
    "                scaler.fit_transform(X_train),\n",
    "                columns=X_train.columns,\n",
    "                index=X_train.index\n",
    "            )\n",
    "\n",
    "            X_test_scaled = pd.DataFrame(\n",
    "                scaler.transform(X_test),\n",
    "                columns=X_test.columns,\n",
    "                index=X_test.index\n",
    "            )\n",
    "\n",
    "\n",
    "            X_train_scaled[\"intercept\"] = 1\n",
    "            X_test_scaled[\"intercept\"] = 1\n",
    "\n",
    "\n",
    "            model = sm.GLM(y_train, X_train_scaled, family=sm.families.Binomial(link=sm.families.links.log())).fit(disp = 0)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            key_y = (covar, i, \"y\")\n",
    "            key_p = (covar, i, \"ypred\")\n",
    "            if key_y not in results_dict:\n",
    "                results_dict[key_y] = pd.Series(dtype=float)\n",
    "                results_dict[key_p] = pd.Series(dtype=float)\n",
    "\n",
    "            results_dict[key_y] = pd.concat([results_dict[key_y], y_test], axis=0)\n",
    "            results_dict[key_p] = pd.concat([results_dict[key_p], y_test_pred], axis=0)\n",
    "\n",
    "            params = model.params\n",
    "            conf = np.exp(model.conf_int())\n",
    "            conf[\"RR\"] = np.exp(params)\n",
    "            conf[\"z\"] = model.tvalues\n",
    "            conf[\"P>|z|\"] = model.pvalues\n",
    "            conf.columns = [\"2.5%\", \"97.5%\", \"RR\", \"z\", \"P>|z|\"]\n",
    "\n",
    "            hl10_stat, hl10_p = hosmer_lemeshow(y_test, y_test_pred, g=10)\n",
    "            hl50_stat, hl50_p = hosmer_lemeshow(y_test, y_test_pred, g=50)\n",
    "            hl100_stat, hl100_p = hosmer_lemeshow(y_test, y_test_pred, g=100)\n",
    "\n",
    "            df_i = conf.loc[i:i].copy()\n",
    "            df_i[\"Feature\"] = i\n",
    "            df_i[\"Covar\"] = covar\n",
    "            df_i[\"boosts\"] = boosts\n",
    "\n",
    "            df_i[\"hl_statistic_10\"] = hl10_stat\n",
    "            df_i[\"hl_pvalue_10\"] = hl10_p\n",
    "            df_i[\"hl_statistic_50\"] = hl50_stat\n",
    "            df_i[\"hl_pvalue_50\"] = hl50_p\n",
    "            df_i[\"hl_statistic_100\"] = hl100_stat\n",
    "            df_i[\"hl_pvalue_100\"] = hl100_p\n",
    "\n",
    "            df_directions_odd_all.append(df_i)\n",
    "\n",
    "            df_c = conf.loc[covar:covar].copy()\n",
    "            df_c[\"Feature\"] = i + \"(\" + covar + \")\"\n",
    "            df_c[\"Exposure\"] = i\n",
    "            df_c[\"Covar\"] = covar\n",
    "            df_c[\"boosts\"] = boosts\n",
    "\n",
    "            df_c[\"hl_statistic_10\"] = hl10_stat\n",
    "            df_c[\"hl_pvalue_10\"] = hl10_p\n",
    "            df_c[\"hl_statistic_50\"] = hl50_stat\n",
    "            df_c[\"hl_pvalue_50\"] = hl50_p\n",
    "            df_c[\"hl_statistic_100\"] = hl100_stat\n",
    "            df_c[\"hl_pvalue_100\"] = hl100_p\n",
    "\n",
    "            df_directions_cov_all.append(df_c)\n",
    "\n",
    "df_directions_odd = pd.concat(df_directions_odd_all, axis=0).reset_index(drop=True)\n",
    "df_directions_cov_odd = pd.concat(df_directions_cov_all, axis=0).reset_index(drop=True)\n",
    "\n",
    "#df_directions_odd, df_directions_cov_odd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150ee0b-dfd8-4312-bcf9-0e3346a39a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = list(vars_) \n",
    "covars = df_directions_odd[\"Covar\"].dropna().unique()\n",
    "\n",
    "idx = pd.MultiIndex.from_product([covars, features], names=[\"Covar\", \"Feature\"])\n",
    "df_directions_odd_f = pd.DataFrame(index=idx)\n",
    "\n",
    "for c in covars:\n",
    "    for f in features:\n",
    "        df_directions_odd_f = summarize_feature_by_covar(\n",
    "            df_directions_odd, c, f, df_directions_odd_f,\n",
    "            stat_cols=(10, 50, 100),\n",
    "            df_offset=2\n",
    "        )\n",
    "\n",
    "df_directions_odd_f = df_directions_odd_f.reset_index()\n",
    "df_directions_odd_f;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26e3d7-2d06-466a-85b9-bc132bad5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for covar in ['total_exposomes', 'sociopolitical', 'social_physical', 'GDP', 'number_leng_inst', 'number_stable_inst', 'distance', 'Migration'] + vars_social_physical + vars_sociopolitical:\n",
    "    c_dfRR = df_directions_odd_f[df_directions_odd_f.Covar == covar]\n",
    "    c_dfRR.to_excel('Results/long/long_RR_-with-slovakia-proficiency_covar-' + covar + '-iter.xlsx')\n",
    "\n",
    "    print(covar)\n",
    "    display(c_dfRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191408f7-8236-4b97-b86c-e49d05c2705d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe9540-2a54-4c73-bea0-ffbd80edb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time.perf_counter() - t0\n",
    "m, s = divmod(dt, 60)\n",
    "print(f\"Tiempo total: {int(m)} min {s:.1f} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
